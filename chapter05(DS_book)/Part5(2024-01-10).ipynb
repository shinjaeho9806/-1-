{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ccb95c-710e-4dbb-9a19-3f198b796d69",
   "metadata": {},
   "source": [
    "* 2023-12-29\n",
    "* 파이토치와 트랜스포머를 활용한 자연어처리와 컴퓨터비전 심층학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91af005c-2f36-4269-ba75-28dee1e1a6ef",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 5장 토큰화\n",
    "* 컴퓨터가 자연어를 이해할 수 있게 나누는 과정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91495468-6058-4506-a930-33478f32de56",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820faa3-a6b5-425c-9de5-8892a44b8378",
   "metadata": {},
   "source": [
    "* [1] 단어 토큰화\n",
    "* 텍스트데이터를 의미있는 단위인 단어로 분리하는 작업\n",
    "* 띄어쓰기나 문장 부호를 활용하여 분리함\n",
    "* 한국어기준 접사, 문장 부호, 오타, 띄어쓰기오류 등에 취약\n",
    "* split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fcbacb2-cb06-483f-8677-b8db80df0bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['현실과', '구분', '불가능한', 'cg.', '시각적', '즐거움은', '최고!', '더불어', 'ost는', '더더욱', '최고!!']\n"
     ]
    }
   ],
   "source": [
    "split_tokens = sentence.split()\n",
    "print(split_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70785762-7cc6-49f1-ad1f-b92ee3af0adb",
   "metadata": {},
   "source": [
    "* [2] 글자 토큰화\n",
    "* 띄어쓰기 뿐아니라 글자 단위로 문장을 나누는 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4348380b-6259-47c4-a6ec-656bae5c3947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['현', '실', '과', ' ', '구', '분', ' ', '불', '가', '능', '한', ' ', 'c', 'g', '.', ' ', '시', '각', '적', ' ', '즐', '거', '움', '은', ' ', '최', '고', '!', ' ', '더', '불', '어', ' ', 'o', 's', 't', '는', ' ', '더', '더', '욱', ' ', '최', '고', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "list_tokens = list(sentence)\n",
    "print(list_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae82be-8c59-403f-9329-45078c7461df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# jamo 변환함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32362294-6c4b-4f2a-9edf-ac536edda701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jamo\n",
      "  Using cached jamo-0.4.1-py3-none-any.whl (9.5 kB)\n",
      "Installing collected packages: jamo\n",
      "Successfully installed jamo-0.4.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install jamo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c95c0-4f08-4910-994e-881ef7b2810d",
   "metadata": {},
   "source": [
    "* jamo_1. 자소단위 토큰화\n",
    "* 자모 변환함수 : jamo.h2j(\"한글문장\")\n",
    "* 한글 호환성 자모 변환함수 : list(jamo.j2hcj(jamo.h2j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cba3a295-6da9-4c2e-a068-dbb681b1c51a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jamo import h2j, j2hcj\n",
    "\n",
    "review = \"현실과 구분 불가능한 cg. 시각적 즐거움은 최고! 더불어 ost는 더더욱 최고!!\" \n",
    "h2j(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3de3afe-91b4-46c8-9531-519cd2bb819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅎㅕㄴㅅㅣㄹㄱㅘ ㄱㅜㅂㅜㄴ ㅂㅜㄹㄱㅏㄴㅡㅇㅎㅏㄴ cg. ㅅㅣㄱㅏㄱㅈㅓㄱ ㅈㅡㄹㄱㅓㅇㅜㅁㅇㅡㄴ ㅊㅚㄱㅗ! ㄷㅓㅂㅜㄹㅇㅓ ostㄴㅡㄴ ㄷㅓㄷㅓㅇㅜㄱ ㅊㅚㄱㅗ!!\n",
      "\n",
      "['ㅎ', 'ㅕ', 'ㄴ', 'ㅅ', 'ㅣ', 'ㄹ', 'ㄱ', 'ㅘ', ' ', 'ㄱ', 'ㅜ', 'ㅂ', 'ㅜ', 'ㄴ', ' ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㄱ', 'ㅏ', 'ㄴ', 'ㅡ', 'ㅇ', 'ㅎ', 'ㅏ', 'ㄴ', ' ', 'c', 'g', '.', ' ', 'ㅅ', 'ㅣ', 'ㄱ', 'ㅏ', 'ㄱ', 'ㅈ', 'ㅓ', 'ㄱ', ' ', 'ㅈ', 'ㅡ', 'ㄹ', 'ㄱ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㅁ', 'ㅇ', 'ㅡ', 'ㄴ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', ' ', 'ㄷ', 'ㅓ', 'ㅂ', 'ㅜ', 'ㄹ', 'ㅇ', 'ㅓ', ' ', 'o', 's', 't', 'ㄴ', 'ㅡ', 'ㄴ', ' ', 'ㄷ', 'ㅓ', 'ㄷ', 'ㅓ', 'ㅇ', 'ㅜ', 'ㄱ', ' ', 'ㅊ', 'ㅚ', 'ㄱ', 'ㅗ', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "decomposed = j2hcj(h2j(review))\n",
    "tokenized = list(decomposed)\n",
    "print(decomposed)\n",
    "print()\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f9fa7-6a85-4976-8b0a-0b8fed63956e",
   "metadata": {},
   "source": [
    "# 형태소 토큰화\n",
    "* 실제로 의미를 가지고 있는 최소의 단위를 형태소라함\n",
    "* 자립 형태소 : 의미를 가지고 있는 형태소   <그, 나, 인사>\n",
    "* 의존 형태소 : 의미를 가지지 않고 다른 형태소와 조합되어 사용되는 형태소 <-는, -에게, -를, ...>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc044fa-6e8d-496b-8d1e-5b37119cfa3e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (1)  konlpy\n",
    "* 한국어 자연어 처리를 위해 개발된 라이브러리로 명사추출, 형태소 분석, 품사 태깅 드으이 기능을 제공\n",
    "* 자바기반이므로  자바개발키트를 설치하여 환경변수로 등록한후 (https://www.oracle.com/java/technologies/downloads/#jdk21-windows)\n",
    "* conda install -c conda-forge jpype1를 실행하여 자바가 파이썬에서도 실행가능하도록 하면 실행가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9664e57-48c9-4f41-a61f-5f93401c9604",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Using cached konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading JPype1-1.5.0-cp39-cp39-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting lxml>=4.1.0 (from konlpy)\n",
      "  Downloading lxml-5.1.0-cp39-cp39-win_amd64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from konlpy) (1.26.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from JPype1>=0.7.0->konlpy) (23.2)\n",
      "Downloading JPype1-1.5.0-cp39-cp39-win_amd64.whl (351 kB)\n",
      "   ---------------------------------------- 0.0/351.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 71.7/351.6 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 351.6/351.6 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading lxml-5.1.0-cp39-cp39-win_amd64.whl (3.9 MB)\n",
      "   ---------------------------------------- 0.0/3.9 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.7/3.9 MB 14.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.4/3.9 MB 12.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.9/3.9 MB 12.1 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 2.4/3.9 MB 11.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 3.0/3.9 MB 11.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.7/3.9 MB 13.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.9/3.9 MB 12.4 MB/s eta 0:00:00\n",
      "Installing collected packages: lxml, JPype1, konlpy\n",
      "Successfully installed JPype1-1.5.0 konlpy-0.6.0 lxml-5.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7455fdff-cb98-4a69-93d3-aca29cd40176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import *\n",
    "sentence = \"무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fe4c91c-0b6a-41ac-93b6-eca58d2c588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출 : ['무엇', '상상', '수', '사람', '무엇', '낼', '수']\n",
      "구문 추출 : ['무엇', '상상', '상상할 수', '상상할 수 있는 사람', '사람']\n",
      "형태소 추출 : ['무엇', '이든', '상상', '할', '수', '있는', '사람', '은', '무엇', '이든', '만들어', '낼', '수', '있다', '.']\n",
      "품사 태깅 : [('무엇', 'Noun'), ('이든', 'Josa'), ('상상', 'Noun'), ('할', 'Verb'), ('수', 'Noun'), ('있는', 'Adjective'), ('사람', 'Noun'), ('은', 'Josa'), ('무엇', 'Noun'), ('이든', 'Josa'), ('만들어', 'Verb'), ('낼', 'Noun'), ('수', 'Noun'), ('있다', 'Adjective'), ('.', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "# Okt 토큰화\n",
    "# SNS 텍스트 데이터를 기반으로 개발됨\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# [1] 명사 추출\n",
    "nouns = okt.nouns(sentence)\n",
    "# [2] 구문 추출\n",
    "phrase = okt.phrases(sentence)\n",
    "# [3] 형태소 추출\n",
    "morphs = okt.morphs(sentence)\n",
    "# [4] 품사 태킹 (형태소, 품사) -> 19가지 품사\n",
    "pos = okt.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"구문 추출 :\", phrase)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\", pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "393c528d-83fb-4f77-9234-79c60a9223f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "명사 추출 : ['무엇', '상상', '수', '사람', '무엇']\n",
      "문장 추출 : 무엇이든 상상할 수 있는 사람은 무엇이든 만들어 낼 수 있다.\n",
      "형태소 추출 : ['무엇', '이', '든', '상상', '하', 'ㄹ', '수', '있', '는', '사람', '은', '무엇', '이', '든', '만들', '어', '내', 'ㄹ', '수', '있', '다', '.']\n",
      "품사 태깅 : [('무엇', 'NNG'), ('이', 'VCP'), ('든', 'ECE'), ('상상', 'NNG'), ('하', 'XSV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('는', 'ETD'), ('사람', 'NNG'), ('은', 'JX'), ('무엇', 'NP'), ('이', 'VCP'), ('든', 'ECE'), ('만들', 'VV'), ('어', 'ECD'), ('내', 'VXV'), ('ㄹ', 'ETD'), ('수', 'NNB'), ('있', 'VV'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "# 꼬꼬마 토큰화\n",
    "# 국립국어원에서 배포한 세종 말뭉치를 기반으로 학습됨\n",
    "\n",
    "kkma = Kkma()\n",
    "\n",
    "# [1] 명사 추출\n",
    "nouns = kkma.nouns(sentence)\n",
    "# [2] 문장 추출\n",
    "sentences = kkma.sentences(sentence)\n",
    "# [3] 형태소 추출\n",
    "morphs = kkma.morphs(sentence)\n",
    "# [4] 품사 태깅 (형태소, 품사) -> 56가지 품사\n",
    "pos = kkma.pos(sentence)\n",
    "\n",
    "print(\"명사 추출 :\", nouns)\n",
    "print(\"문장 추출 :\", sentence)\n",
    "print(\"형태소 추출 :\", morphs)\n",
    "print(\"품사 태깅 :\",pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ddd058-4946-432c-b097-5b02011bdafb",
   "metadata": {},
   "source": [
    "* 토큰화 모형들의 품사들은 책 p.241 ~ 242 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845d2837-62a4-481b-9d2d-28a08f138575",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (2) NLTK\n",
    "* 자연어 처리를 위하여 개발된 라이브러리\n",
    "* 토큰화나 품사태깅을 수행하기위해서는 해당 작업을 수행할 수 있는 패키지나 모델을 다운로드 해야함\n",
    "* 대규모의 영어 말뭉치인 Tree bank를 기반으로 학습된 모형인 punkt(통계기반)와 averaged_perceptron_tagger(퍼셉트론 기반)를 이용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96b9a63d-134f-41dc-b158-257d7a0ceb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "736c7e86-5daa-42cd-8188-2bc3adbbe4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "from nltk import tag\n",
    "\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5694b138-f9bd-465c-997e-bc1d1ed31311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_tokens : ['Those', 'who', 'can', 'imagine', 'anything', ',', 'can', 'create', 'the', 'impossible', '.']\n",
      "sent_tokens : ['Those who can imagine anything, can create the impossible.']\n"
     ]
    }
   ],
   "source": [
    "# 단어 토큰화 -> punkt를 기반으로\n",
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "# 문장 토큰화 -> 구두점을 기준으로\n",
    "sent_tokens = tokenize.sent_tokenize(sentence)\n",
    "\n",
    "print(f\"word_tokens : {word_tokens}\")\n",
    "print(f\"sent_tokens : {sent_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2ce6063-85a1-4b7a-93d4-9c1c3cc8a55b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Those', 'DT'), ('who', 'WP'), ('can', 'MD'), ('imagine', 'VB'), ('anything', 'NN'), (',', ','), ('can', 'MD'), ('create', 'VB'), ('the', 'DT'), ('impossible', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenize.word_tokenize(sentence)\n",
    "# averaged_perceptron_tagger를 이용하여 태킹 -> 35가지 태깅가능\n",
    "pos = tag.pos_tag(word_tokens)\n",
    "\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814689d-bc7f-40c8-92ce-4981ccadfb72",
   "metadata": {},
   "source": [
    "* averaged_perceptron_tagger의 품사태그종류는 책 p.244~245확인 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e6258-6a48-4cc6-b025-17dc4c5bc86f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (3) spaCy\n",
    "* 머신러닝 깁나의 자연어처리 라이브러리(빠른 속도와 높은 정확도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87fcaf54-e86c-436f-99ff-5a94d2661732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e233552-091d-4dff-9838-5c284e2adc20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 653.6 kB/s eta 0:00:20\n",
      "     - -------------------------------------- 0.5/12.8 MB 4.7 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 1.1/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 9.7 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 2.8/12.8 MB 11.9 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.0/12.8 MB 14.1 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.4/12.8 MB 16.6 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.2/12.8 MB 19.2 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.4/12.8 MB 22.2 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.1/12.8 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 36.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.7.1\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88598240-16d8-4ca0-8ee1-41fbbe1f13df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sentence = \"Those who can imagine anything, can create the impossible.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e966758-0fb5-465e-bbf8-b1ea6645a7a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PRON  - DT ] : Those\n",
      "[PRON  - WP ] : who\n",
      "[AUX   - MD ] : can\n",
      "[VERB  - VB ] : imagine\n",
      "[PRON  - NN ] : anything\n",
      "[PUNCT - ,  ] : ,\n",
      "[AUX   - MD ] : can\n",
      "[VERB  - VB ] : create\n",
      "[DET   - DT ] : the\n",
      "[ADJ   - JJ ] : impossible\n",
      "[PUNCT - .  ] : .\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(sentence)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"[{token.pos_:5} - {token.tag_:3}] : {token.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cd3eb5-1d01-4e33-832b-ab02961a13f7",
   "metadata": {},
   "source": [
    "* token객체의 속성들\n",
    "* pos_ : 기본 품사 속성\n",
    "* tag_ : 세분화 품사 속성  <자세한 태그는 책 p.247~248을 참조>\n",
    "* text : 원본 텍스트 데이터\n",
    "* text_with_ws : 공백을 포함하는 텍스트 데이터\n",
    "* vector : 벡터\n",
    "* vector norm : 벡터놈"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f18c9b-e0a6-4605-8134-4bd3c0e5a6d9",
   "metadata": {},
   "source": [
    "# 하위 단어 토큰화\n",
    "* 하나의 단어가 빈번하게 사용되는 하위단어의 조합으로 나누어 토큰화를 진행하는 방식\n",
    "* ex. reinforement -> rein, force, ment\n",
    "* 바이트 페어 인코딩, 워드피스, 유니그램 ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f134676-fb4a-411e-b65e-95c28c05ca4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# (1) 바이트 페어 인코딩\n",
    "* 연속된 글자 쌍이 더 이상 나타나지 않거나, 정해진 어휘 사전 크기에 도달할 때까지 조합탐지와 부호화를 반복하여, 이 과정에서 자주 등장하는 단어는 하나의 토큰으로 토큰화하고, 덜 등장하는 단어는 여러 토큰의 조합으로 표현된다.\n",
    "* p.251~252 예시\n",
    "\n",
    "## Sentencepiece와 Korpora\n",
    "*  Sentencepice : 구글에서 개발한 하위 단어 토크나이저 라이브러리\n",
    "*  Korpora : 국립국어원이나 AI Hub애서 제공하는 말뭉치를 사용할 수 있게 개발된 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48aab09a-a1b0-4ade-9c7e-d49a745acc11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp39-cp39-win_amd64.whl (977 kB)\n",
      "     ---------------------------------------- 0.0/977.6 kB ? eta -:--:--\n",
      "     - ----------------------------------- 30.7/977.6 kB 660.6 kB/s eta 0:00:02\n",
      "     ------------- ------------------------ 358.4/977.6 kB 4.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- --- 880.6/977.6 kB 7.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 977.6/977.6 kB 6.9 MB/s eta 0:00:00\n",
      "Collecting Korpora\n",
      "  Using cached Korpora-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting dataclasses>=0.6 (from Korpora)\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from Korpora) (1.26.0)\n",
      "Requirement already satisfied: tqdm>=4.46.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from Korpora) (4.66.1)\n",
      "Requirement already satisfied: requests>=2.20.0 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from Korpora) (2.31.0)\n",
      "Collecting xlrd>=1.2.0 (from Korpora)\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "     ---------------------------------------- 0.0/96.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 96.5/96.5 kB ? eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests>=2.20.0->Korpora) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests>=2.20.0->Korpora) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests>=2.20.0->Korpora) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from requests>=2.20.0->Korpora) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\ds_env\\lib\\site-packages (from tqdm>=4.46.0->Korpora) (0.4.6)\n",
      "Installing collected packages: sentencepiece, dataclasses, xlrd, Korpora\n",
      "Successfully installed Korpora-0.2.0 dataclasses-0.6 sentencepiece-0.1.99 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece Korpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed0f1dee-8b39-48e7-91a9-b2b26d96ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Korpora import Korpora"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f24546-67e4-4269-98f5-0cb1491c427f",
   "metadata": {},
   "source": [
    "* sentencepiece 라이브러리를 통해 바이트 페어 인코딩을 수행하는 토크나이저 모델을 학습시키기 위해,\n",
    "* 2017.08 ~ 2019.03까지 청와대 청원 게시판에 올라온 청원 말뭉치를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5731ea6-9fb4-46a6-b90a-557219a053d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[korean_petitions] download petitions_2017-08: 1.84MB [00:00, 7.43MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-09: 20.4MB [00:00, 44.1MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-10: 12.0MB [00:00, 29.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-11: 28.4MB [00:00, 34.2MB/s]                                                \n",
      "[korean_petitions] download petitions_2017-12: 29.0MB [00:00, 55.5MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-01: 43.9MB [00:00, 68.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-02: 33.8MB [00:00, 53.6MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-03: 34.3MB [00:02, 12.3MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-04: 35.5MB [00:00, 61.1MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-05: 37.5MB [00:00, 62.0MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-06: 37.8MB [00:00, 64.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-07: 40.5MB [00:00, 69.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-08: 39.8MB [00:00, 64.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-09: 36.1MB [00:01, 29.7MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-10: 38.1MB [00:00, 63.6MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-11: 37.7MB [00:00, 66.6MB/s]                                                \n",
      "[korean_petitions] download petitions_2018-12: 33.0MB [00:00, 57.0MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-01: 34.8MB [00:00, 60.8MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-02: 30.8MB [00:00, 50.4MB/s]                                                \n",
      "[korean_petitions] download petitions_2019-03: 34.9MB [00:00, 60.2MB/s]                                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청원 시작일 : 2017-08-25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 말뭉치 데이터 다운\n",
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "dataset = corpus.train\n",
    "petition = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a228075-e7c9-4fc0-9015-39810976994b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "청원 시작일 : 2017-08-25\n",
      "청원 종료일 : 2017-09-24\n",
      "청원 동의 수 : 88\n",
      "청원 범주 : 육아/교육\n",
      "청원 시작일 : 학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'청원 시작일 : 안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"청원 시작일 : {petition.begin}\")\n",
    "print(f\"청원 종료일 : {petition.end}\")\n",
    "print(f\"청원 동의 수 : {petition.num_agree}\")\n",
    "print(f\"청원 범주 : {petition.category}\")\n",
    "print(f\"청원 시작일 : {petition.title}\")\n",
    "f\"청원 시작일 : {petition.text[:30]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bf69b2d-2f88-4c8f-bf88-516b14cb6fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoreanPetitions.train: size=433631\n",
       "  - KoreanPetitions.train.texts : list[str]\n",
       "  - KoreanPetitions.train.categories : list[str]\n",
       "  - KoreanPetitions.train.num_agrees : list[int]\n",
       "  - KoreanPetitions.train.begins : list[str]\n",
       "  - KoreanPetitions.train.ends : list[str]\n",
       "  - KoreanPetitions.train.titles : list[str]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128a67f8-7290-4c6b-960b-4507238ac46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KoreanPetition(text=\"안녕하세요. 현재 사대, 교대 등 교원양성학교들의 예비교사들이 임용절벽에 매우 힘들어 하고 있는 줄로 압니다. 정부 부처에서는 영양사의 영양'교사'화, 폭발적인 영양'교사' 채용, 기간제 교사, 영전강, 스강의 무기계약직화가 그들의 임용 절벽과는 전혀 무관한 일이라고 주장하고 있지만 조금만 생각해보면 전혀 설득력 없는 말이라고 생각합니다. 학교 수가 같고, 학생 수가 동일한데 영양교사와 기간제 교사, 영전강 스강이 학교에 늘어나게 되면 당연히 정규 교원의 수는 줄어들게 되지 않겠습니까? 기간제 교사, 영전강, 스강의 무기계약직화, 정규직화 꼭 전면 백지화해주십시오. 백년대계인 국가의 교육에 달린 문제입니다. 단순히 대통령님의 일자리 공약, 81만개 일자리 창출 공약을 지키시고자 돌이킬 수 없는 실수는 하지 않으시길 바랍니다. 세계 어느 나라와 비교해도, 한국 교원의 수준과 질은 최고 수준입니다. 고등교육을 받고 어려운 국가 고시를 통과해야만 대한민국 공립 학교의 교단에 설 수 있고, 이러한 과정이 힘들기는 하지만 교원들이 교육자로서의 사명감과 자부심을 갖고 교육하게 되는 원동력이기도 합니다. 자격도 없는 비정규 인력들을 일자리 늘리기 명목 하에 학교로 들이게 되면, 그들이 무슨 낯으로 대한민국이 '공정한 사회' 라고 아이들에게 가르칠 수 있겠습니까? 그들이 가르치는 것을 학부모와 학생들이 납득할 수 있겠으며, 학생들은 공부를 열심히 해야하는 이유를 찾을 수나 있겠습니까? 열심히 안 해도 떼 쓰면 되는 세상이라고 생각하지 않겠습니까? 영양사의 영양교사화도 재고해주십시오. 영양사분들 정말 너무나 고마운 분들입니다. 학생들의 건강과 영양? 당연히 성장기에 있는 아이들에게 필수적이고 중요한 문제입니다. 하지만 이들이 왜 교사입니까. 유래를 찾아 볼 수 없는 영양사의 '교사'화. 정말 대통령님이 생각하신 아이디어라고 믿기 싫을 정도로 납득하기 어렵습니다. 중등은 실과교과 교사가 존재하지요? 초등 역시 임용 시험에 실과가 포함돼 있으며 학교 현장에서도 정규 교원이 직접 실과 과목을 학생들에게 가르칩니다. 영양'교사', 아니 영양사가 학생들에게 실과를 가르치지 않습니다. 아니 그 어떤 것도 가르치지 않습니다. 올해 대통령님 취임 후에 초등, 중등 임용 티오가 초전박살 나는 동안 영양'교사' 티오는 폭발적으로 확대된 줄로 압니다. 학생들의 교육을 위해 정말 교원의 수를 줄이고, 영양 교사의 수를 늘리는 것이 올바른 해답인지 묻고 싶습니다. 마지막으로 교원 당 학생 수. 이 통계도 제대로 내주시기 바랍니다. 다른 나라들은 '정규 교원', 즉 담임이나 교과 교사들로만 통계를 내는데(너무나 당연한 것이지요) 왜 한국은 보건, 영양, 기간제, 영전강, 스강 까지 다 포함해서 교원 수 통계를 내는건가요? 이런 통계의 장난을 통해 OECD 평균 교원 당 학생 수와 거의 비슷한 수준에 이르렀다고 주장하시는건가요? 학교는 교육의 장이고 학생들의 공간이지, 인력 센터가 아닙니다. 부탁드립니다. 부디 넓은 안목으로 멀리 내다봐주시길 간곡히 부탁드립니다.\", category='육아/교육', num_agree=88, begin='2017-08-25', end='2017-09-24', title='학교는 인력센터, 취업센터가 아닙니다. 정말 간곡히 부탁드립니다.')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "petition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da77691c-b0ad-473d-b393-3b5dbe1a4c57",
   "metadata": {},
   "source": [
    "* 학습데이터 세트 생성(메모장)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c43c6e-42c1-4b2a-80e7-404a68fdad16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
      "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
      "\n",
      "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
      "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
      "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
      "\n",
      "    # Description\n",
      "    Author : Hyunjoong Kim lovit@github\n",
      "    Repository : https://github.com/lovit/petitions_archive\n",
      "    References :\n",
      "\n",
      "    청와대 국민청원 게시판의 데이터를 월별로 수집한 것입니다.\n",
      "    청원은 게시판에 글을 올린 뒤, 한달 간 청원이 진행됩니다.\n",
      "    수집되는 데이터는 청원종료가 된 이후의 데이터이며, 청원 내 댓글은 수집되지 않습니다.\n",
      "    단 청원의 동의 개수는 수집됩니다.\n",
      "    자세한 내용은 위의 repository를 참고하세요.\n",
      "\n",
      "    # License\n",
      "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
      "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
      "\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2017-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2017-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2017-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2017-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2017-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-03\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-04\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-05\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-06\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-07\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-08\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-09\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-10\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-11\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2018-12\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2019-01\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2019-02\n",
      "[Korpora] Corpus `korean_petitions` is already installed at C:\\Users\\user\\Korpora\\korean_petitions\\petitions_2019-03\n"
     ]
    }
   ],
   "source": [
    "corpus = Korpora.load(\"korean_petitions\")\n",
    "petitions = corpus.get_all_texts()\n",
    "with open(\"./corpus.txt\", \"w\", encoding = \"utf-8\") as f:\n",
    "    for petition in petitions:\n",
    "        # 하나의 문장이 끝나면 다음 문장으로 넘어감.\n",
    "        f.write(petition + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8aa310-fdb5-4988-90bd-abc2a8287a91",
   "metadata": {},
   "source": [
    "* 토크나이저 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c722430b-0201-46a2-979c-4a43a3023e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentencepiece import SentencePieceTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc2a45-8fbd-4d1e-a968-b0d72f2ab0f2",
   "metadata": {},
   "source": [
    "\n",
    "SentencePieceTrainer.Train(\n",
    "    \"--input : 말뭉치 텍스트 파일의 경로\\\n",
    "    --model_prefix : 모델 파일 이름\\\n",
    "    --vocab_size : 어휘 사전 크기\\\n",
    "    --model_type : 토크나이저 알고리즘\\\n",
    "    --max_sentence_length : 최대 문장 길이\"\n",
    ")\n",
    "* 추가적인 내용은 https://github.com/google/sentencepiece/blob/master/doc/options.md 참조\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae80c098-993d-4a60-aaea-1be80a399e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SentencePieceTrainer.Train( \n",
    "    \"--input=./corpus.txt\\\n",
    "    --model_prefix=petition_bpe\\\n",
    "    --vocab_size=8000 model_type=bpe\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d978fb8-9686-49b5-871b-f5a8ae553206",
   "metadata": {},
   "source": [
    "* 토크나이저 모델과 어휘 사전 파일을 활용해 바이트 페어 인코딩을 수행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c857347-47f0-4a78-bc85-4b32ec8526df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"petition_bpe.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88a980ea-3ab4-423b-b51e-6e33a0c06385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentences = [\"이렇게 입력값을 리스트로 받아서\",\"쉽게 토크나이저를 사용할 수 있답니다 \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2d76952-ba57-4785-8d99-7fb20d417347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 토큰화 : ['▁안녕하세요', ',', '▁토', '크', '나', '이', '저', '가', '▁잘', '▁학', '습', '되었', '군요', '!']\n",
      "여러 문장 토큰화 : [['▁이렇게', '▁입', '력', '값을', '▁리', '스트', '로', '▁받아서'], ['▁쉽게', '▁토', '크', '나', '이', '저', '를', '▁사용할', '▁수', '▁있', '답니다']]\n"
     ]
    }
   ],
   "source": [
    "# encode_as_piece 문장을 토큰화\n",
    "tokenizer_sentence = tokenizer.encode_as_pieces(sentence)\n",
    "tokenizer_sentences = tokenizer.encode_as_pieces(sentences)\n",
    "print(f\"단일 문장 토큰화 : {tokenizer_sentence}\")\n",
    "print(f\"여러 문장 토큰화 : {tokenizer_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1dedf4af-aa26-4fa0-affe-55185c0bf2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 정수 인코딩 : [667, 6553, 994, 6880, 6544, 6513, 6590, 6523, 161, 110, 6554, 872, 787, 6648]\n",
      "여러 문장 정수 인코딩 : [[372, 182, 6677, 4433, 1772, 1613, 6527, 4162], [1681, 994, 6880, 6544, 6513, 6590, 6536, 5852, 19, 5, 2639]]\n"
     ]
    }
   ],
   "source": [
    "# encode_as_ids 토큰을 정수로 인코딩\n",
    "encoded_sentence = tokenizer.encode_as_ids(sentence)\n",
    "encoded_sentences = tokenizer.encode_as_ids(sentences)\n",
    "print(f\"단일 문장 정수 인코딩 : {encoded_sentence}\")\n",
    "print(f\"여러 문장 정수 인코딩 : {encoded_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d350b9b-f1fe-4309-9e9e-a556aed901a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩에서 단일 문장 변환 : 안녕하세요, 토크나이저가 잘 학습되었군요!\n",
      "하위 단어 토큰에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"
     ]
    }
   ],
   "source": [
    "# decode_ids\n",
    "decode_ids = tokenizer.decode_ids(encoded_sentence)\n",
    "# decode_pieces\n",
    "decode_pieces = tokenizer.decode_pieces(encoded_sentences)\n",
    "\n",
    "print(f\"정수 인코딩에서 단일 문장 변환 : {decode_ids}\")\n",
    "print(f\"하위 단어 토큰에서 문장 변환 : {decode_pieces}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888d78e-8555-4aca-84d9-3309221809e8",
   "metadata": {},
   "source": [
    "*  어휘 사전 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f829a999-6658-47a5-90c0-b328f9a338b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '<unk>'), (1, '<s>'), (2, '</s>'), (3, '니다'), (4, '▁이')]\n",
      "vocab_size : 8000\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "tokenizer = SentencePieceProcessor()\n",
    "tokenizer.load(\"petition_bpe.model\")\n",
    "\n",
    "vocab = {idx : tokenizer.id_to_piece(idx) for idx in range(tokenizer.get_piece_size())}\n",
    "print(list(vocab.items())[:5])\n",
    "print(f\"vocab_size : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf59ae8-4c02-45c7-9f11-6d8afde7f74a",
   "metadata": {},
   "source": [
    "* \\<s> : 문장의 시작지점\n",
    "* \\<\\s> : 문장의 종료지점\n",
    "* \\<unk> : OOV 발생 시 매핑되는 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207e9de-06f3-4c3b-b2aa-c20749a7f5d3",
   "metadata": {},
   "source": [
    "# (2) 워드피스\n",
    "* 워드피스 토크나이저는 바이트 페어 인코딩 토크나이저와 유사한 방법으로 학습\n",
    "* 빈도 기반이 아닌 확률 기반으로 글자 쌍을 병합\n",
    "* 모델이 새로운 하위 단어를 생성할 때, 이전 하위단어와 함께 나타날 확률을 게산해 가장 높은 확률을 가진 하위 단어를 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff69aaf-c3b3-4f46-9629-734e16cadd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f() : 빈도를 나타낸는 함수\n",
    "# x,y : 병합하려는 하위 단어\n",
    "# f(x,y)/f(x)f(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0adeb25-9ccb-40e7-aa76-fa782887de0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tokenizer 학습하기\n",
    "* 정규롸 : 일관된 형식으로 텍스트를 표준화\n",
    "-> 입부문자 대체/제거, 불필요한 공백 제거, 대소문자 변환, 유니코드 정규화, 구두점 처리, 특수문자 처리\n",
    "* 사전토큰화 : 토큰화하기 전에 단어와 같은 작은 단어로 사전에 나누는 것\n",
    "-> 공백 혹은 구두점을 기준으로 입력 문장을 나눠 텍스트 데이터를 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb2588cb-5cb1-4aad-b40f-ea7d91a0d6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.normalizers import Sequence, NFD, Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# [1] 토크나이저로 워드피스 모델을 호출\n",
    "tokenizer = Tokenizer(WordPiece())\n",
    "# [2] 정규화 방식과 사전 토큰화 방식을 설정\n",
    "# normalizers모듈에 포함된 클래스를 시퀀스형식으로 인스턴스에 전달\n",
    "# NFD : 유니코드 정규화\n",
    "# Lowercase : 소문자 변환\n",
    "# 책 p. 260\n",
    "tokenizer.normalizer = Sequence([NFD(), Lowercase()])\n",
    "# [3] 사전 토큰화 방식도 모듈에 포함된 클래스를 불러와 적용\n",
    "# 공백과 구두점을 기준으로 분리\n",
    "# 책 p. 262\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "# [4]  정규화 방식과 사전 토큰화가 완료되었음\n",
    "# 훈련메서드를 통해 사용하려는 데이터세트 경로를 전달\n",
    "tokenizer.train(['./corpus.txt'])\n",
    "# [5] 학습이 완료 후 결과를 save로 저장\n",
    "tokenizer.save('./models/petition_wordpiece.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd944de2-49ea-4983-976e-93d3a31eeebc",
   "metadata": {},
   "source": [
    "# 학습이 완료된 Tokenizer 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98d1e54b-b779-4d8b-b570-6344561535bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.decoders import WordPiece as WordPieceDecoder\n",
    "\n",
    "# [1] 토크나이저 객체를 생성\n",
    "tokenizer = Tokenizer.from_file('./models/petition_wordpiece.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a03e27b6-aa78-4597-8f3e-6512ed781fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] 디코더를 워드피스디코더로 지정\n",
    "tokenizer.decoder = WordPieceDecoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c470ef7f-158f-4ac2-a920-b2f8f73b49f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"안녕하세요, 토크나이저가 잘 학습되었군요!\"\n",
    "sentenecs = [\"이렇게 입력값을 리스트로 받아서\", \"쉽게 토크나이저를 사용할 수 있답니다\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4d156e55-77a7-4156-96cc-ab09384f7020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [3] 문장을 토큰화\n",
    "encoded_sentence = tokenizer.encode(sentence)\n",
    "encoded_sentences = tokenizer.encode_batch(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3fc4e4bc-32d2-4ea2-9ad0-7f6d7e71bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "인코더 형식 : <class 'tokenizers.Encoding'>\n",
      "인코더 형식 : <class 'tokenizers.Encoding'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"인코더 형식 : {type(encoded_sentence)}\")\n",
    "print(f\"인코더 형식 : {type(encoded_sentences[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "460a83d2-44f1-4258-b7f1-cfc36f0bceb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 토큰화 : ['안녕하세요', ',', '토', '##크', '##나이', '##저', '##가', '잘', '학습', '##되었', '##군요', '!']\n"
     ]
    }
   ],
   "source": [
    "print(f\"단일 문장 토큰화 : {encoded_sentence.tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2446fb59-546b-4fe1-8097-7decfd3f2a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여러 문장 토큰화 : [['이렇게', '입력', '##값을', '리스트', '##로', '받아서'], ['쉽게', '토', '##크', '##나이', '##저', '##를', '사용할', '수', '있다', '##ᆸ니다']]\n"
     ]
    }
   ],
   "source": [
    "print(f\"여러 문장 토큰화 : {[enc.tokens for enc in encoded_sentences]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09144c44-d52e-4c0b-8423-7dee54ef2b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단일 문장 정수 인코딩 : [8760, 11, 8693, 8415, 16269, 7536, 7488, 7842, 15016, 8670, 8734, 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"단일 문장 정수 인코딩 : {encoded_sentence.ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a69de5f-607b-4479-a74b-e1fc1c85e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "여러 문장 정수 인코딩 : [[8187, 19643, 13834, 28119, 7495, 12607], [9739, 8693, 8415, 16269, 7536, 7510, 14129, 7562, 8157, 7489]]\n"
     ]
    }
   ],
   "source": [
    "print(f\"여러 문장 정수 인코딩 : {[enc.ids for enc in encoded_sentences]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4751b1dd-5dff-4053-b385-a00d4dfdeb90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩에서 문장 변환 : 안녕하세요, 토크나이저가 잘 학습되었군요!\n"
     ]
    }
   ],
   "source": [
    "print(f\"정수 인코딩에서 문장 변환 : {tokenizer.decode(encoded_sentence.ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5807f01-1b92-45ca-b28f-158fb468b632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩에서 문장 변환 : ['이렇게 입력값을 리스트로 받아서', '쉽게 토크나이저를 사용할 수 있답니다']\n"
     ]
    }
   ],
   "source": [
    "print(f\"정수 인코딩에서 문장 변환 : {[tokenizer.decode(enc.ids) for enc in encoded_sentences]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d6284-e175-4ddc-8e76-b6dfdd5e2f24",
   "metadata": {},
   "source": [
    "# END(2024-01-10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS_kernel",
   "language": "python",
   "name": "ds_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
